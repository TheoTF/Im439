{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2db37d",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import GlobalConfig\n",
    "from dataloader import TimeSeriesDataset\n",
    "from optuna_config import OptunaOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd20d8f",
   "metadata": {},
   "source": [
    "# Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f989d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_218 = pd.read_parquet(r\"Datasets/df_ETL_VI_5_Maint_2183731.parquet\")\n",
    "df_214 = pd.read_parquet(r\"Datasets/df_ETL_VI_5_Maint_2143372.parquet\")\n",
    "\n",
    "sensor_columns = ['UA_Z_AR', 'UA_Z_AL', 'UA_Z_BR', 'UA_Z_BL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2189a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop useless columns\n",
    "df_218.drop(['DQ_WeightClass', 'DQ_V_Low', 'DQ_V_High',\n",
    "       'DQ_SS_Sparse_Vals', 'DQ_Z_Cluster_V', 'DQ_LevelShift_V',\n",
    "       'DQ_P_Peaks_Width', 'DQ_V_Peaks_Width', 'DQ_P_Peaks_UA_Width',\n",
    "       'DQ_V_Peaks_UA_Width', 'DQ_P_Greater_110', 'DQ_Trend_UA',\n",
    "       'DQ_LevelShift_UA', 'DQ_SS_Z_AR_Loose', 'DQ_SS_Z_AL_Loose',\n",
    "       'DQ_SS_Z_BR_Loose', 'DQ_SS_Z_BL_Loose', 'DQ_Score',\n",
    "       'WagonNumber', 'Year', 'Month', 'Week_Num', 'Day',\n",
    "       'Latitude', 'Longitude','Elevation','DQ_Line',\n",
    "       'Contaminated_Data', 'DQ_Problems',\n",
    "       'Region', 'Velocity', 'VelClass','Eh', 'Element', 'Haversini_KmIni',\n",
    "       'Haversini_KmFim', 'KmReference', 'Radius', 'TrackType', 'CurveClass',\n",
    "       'Bridge', 'Tunnel', 'Transition', 'Patios', 'MaterialWeight',\n",
    "       'TotalWeight', 'WeightClass', 'Maint_label', 'Detection Date',\n",
    "       'End of Maint. Date', 'Description', 'Symptom', 'Cause', 'DefectType',\n",
    "       'DefectKmIni', 'DefectKmFim', 'DefectExtension', 'MaintDistance',\n",
    "       'SS_Z_Filt_AR', 'SS_Z_Filt_AL', 'SS_Z_Filt_BR', 'SS_Z_Filt_BL',\n",
    "       'Pressure', 'Voltage', 'SuspTravel_L', 'SuspTravel_R', 'SuspTravel',\n",
    "       'Front_Bounce_SS', 'Back_Bounce_SS', 'Bounce', 'Front_Roll_SS',\n",
    "       'Back_Roll_SS', 'Roll', 'UA_Z_L', 'UA_Z_R', 'UA_Z_Max', 'UA_Z_Abs_Max'\n",
    "       ], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "df_218['TripNumber'] = df_218['TripNumber'].astype(int)\n",
    "df_218.rename(columns={'Haversini_Linha':'Line'}, inplace = True)\n",
    "df_218.rename(columns={'TripNumber':'Trip'}, inplace = True)\n",
    "df_218[\"Line\"] = df_218[\"Line\"].str.extract(r\"(\\d+)\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22827bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop useless columns\n",
    "df_214.drop(['DQ_WeightClass', \n",
    "       'WagonNumber', 'Year', 'Month', 'Week_Num', 'Day',\n",
    "       'Latitude', 'Longitude','Elevation','DQ_Line',\n",
    "       'Contaminated_Data', 'DQ_Problems',\n",
    "       'Region', 'Velocity', 'VelClass','Eh', 'Element', 'Haversini_KmIni',\n",
    "       'Haversini_KmFim', 'KmReference', 'Radius', 'TrackType', 'CurveClass',\n",
    "       'Bridge', 'Tunnel', 'Transition', 'Patios', 'MaterialWeight',\n",
    "       'TotalWeight', 'WeightClass', 'Maint_label', 'Detection Date',\n",
    "       'End of Maint. Date', 'Description', 'Symptom', 'Cause', 'DefectType',\n",
    "       'DefectKmIni', 'DefectKmFim', 'DefectExtension', 'MaintDistance',\n",
    "       'SS_Z_Filt_AR', 'SS_Z_Filt_AL', 'SS_Z_Filt_BR', 'SS_Z_Filt_BL',\n",
    "       'Pressure', 'Voltage', 'SuspTravel_L', 'SuspTravel_R', 'SuspTravel',\n",
    "       'Front_Bounce_SS', 'Back_Bounce_SS', 'Bounce', 'Front_Roll_SS',\n",
    "       'Back_Roll_SS', 'Roll', 'UA_Z_L', 'UA_Z_R', 'UA_Z_Max', 'UA_Z_Abs_Max',\n",
    "       'KmIni', 'KmFim'\n",
    "       ], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "df_214['TripNumber'] = df_214['TripNumber'].astype(int)\n",
    "df_214.rename(columns={'Haversini_Linha':'Line'}, inplace = True)\n",
    "df_214.rename(columns={'TripNumber':'Trip'}, inplace = True)\n",
    "df_214[\"Line\"] = df_214[\"Line\"].str.extract(r\"(\\d+)\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68580752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_timestamps(df):\n",
    "    # Step 1: strip the date, keep only the time\n",
    "    df['Timestamp'] = df['Timestamp'].str[11:]\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%H:%M:%S')\n",
    "\n",
    "    grouped = df.groupby(['Line', 'Trip'])\n",
    "    new_groups = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        group = group.sort_values(\"Distance\").copy()\n",
    "\n",
    "        if len(group) < 2:\n",
    "            continue  # or handle single-row case as needed\n",
    "\n",
    "        t0 = group['Timestamp'].iloc[0]\n",
    "        t1 = group['Timestamp'].iloc[1]\n",
    "        diff = (t1 - t0).total_seconds()\n",
    "\n",
    "        delta = (group['Timestamp'] - t0).dt.total_seconds()\n",
    "\n",
    "        if diff >= 0:\n",
    "            delta = delta.where(delta >= 0, delta + 86400)\n",
    "        else:\n",
    "            delta = -delta\n",
    "            delta = delta.where(delta >= 0, delta + 86400)\n",
    "\n",
    "        group['Timestamp'] = delta.astype(int)\n",
    "        new_groups.append(group)\n",
    "\n",
    "    # Concatenate groups and sort within each trip/line by Timestamp\n",
    "    result_df = pd.concat(new_groups)\n",
    "    result_df = result_df.sort_values([\"Line\", \"Trip\", \"Timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    # Overwrite original df in-place\n",
    "    df.loc[:, :] = result_df.values\n",
    "    \n",
    "# Theo nao usa essa funcao ai\n",
    "# normalize_timestamps(df_218)\n",
    "# normalize_timestamps(df_214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_214['Timestamp'] = pd.to_datetime(df_214['Timestamp'])\n",
    "df_218['Timestamp'] = pd.to_datetime(df_218['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71078359",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_218"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e74538",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = GlobalConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55086cf",
   "metadata": {},
   "source": [
    "For wagon 218, we will filter out any point with DQ_Score_Normalized < 0.70, since it will be sed as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56007c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_218 = df_218[df_218['DQ_Score_Normalized'] > 0.70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(global_config, df_218)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580005c",
   "metadata": {},
   "source": [
    "For wagon 214, things are a little more complicated. We will need to filter out the worst problems for test and leave some of them for validation. For validation, we will also need to construct pairs of abnormal and normal examples to check if the model is learning to separate real cases from the synthetic augumentations.\n",
    "\n",
    "We will split the dataset using the worst problems: \"DQ_Z_Cluster\", \"DQ_Trend_UA\", \"DQ_V_High\", \"DQ_P_Greater_110\", \"DQ_P_Peaks_UA_Width\" and \"DQ_V_Peaks_UA_Width\". If any data point contains at leats one of these problems, we will assign it to the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d174dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_214.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29023793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_214_test = df_214[(df_214['DQ_Z_Cluster'] == 1) | \n",
    "                     (df_214['DQ_Trend_UA'] == 1) |\n",
    "                     (df_214['DQ_V_High'] == 1) |\n",
    "                     (df_214['DQ_P_Greater_110'] == 1) |\n",
    "                     (df_214['DQ_P_Peaks_UA_Width'] == 1) |\n",
    "                     (df_214['DQ_V_Peaks_UA_Width'] == 1)]\n",
    "\n",
    "df_214_val = df_214[~df_214.index.isin(df_214_test.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43105fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_214_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_214_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_214_val_dataset = TimeSeriesDataset(global_config, df_214_val)\n",
    "len(df_214_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_214_test_dataset = TimeSeriesDataset(global_config, df_214_test)\n",
    "len(df_214_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03544bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_normal = df_214_val_dataset[:2000]\n",
    "val_dataset_abnormal = df_214_test_dataset[:2000]\n",
    "test_dataset = df_214_test_dataset[2000:] + df_214_test_dataset[2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c227c",
   "metadata": {},
   "source": [
    "We will get 2000 windows for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68408788",
   "metadata": {},
   "source": [
    "# Optuna Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75854a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = OptunaOptimizer(X_train=train_dataset,\n",
    "                       X_val_normal=val_dataset_normal,\n",
    "                       X_val_abnormal=val_dataset_abnormal,\n",
    "                       X_test=test_dataset,\n",
    "                       exp_name=\"Almost_TFC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9aeab7",
   "metadata": {},
   "source": [
    "#TODO: Define a Pruner for the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", study_name=optim.exp_name, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ede69",
   "metadata": {},
   "source": [
    "#TODO: Fix the difference betwwen input type (double) and bias (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(optim.objective, \n",
    "               n_trials=1, \n",
    "               n_jobs=1,\n",
    "               show_progress_bar=True\n",
    "               )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vagao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
